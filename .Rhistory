<<<<<<< HEAD
devtools::install_github("robjhyndman/forecast")
install.packages('devtools')
library(devtools)
devtools::install_github("robjhyndman/forecast")
devtools::install_github("robjhyndman/forecast")
??arima
data("AirPassengers")
data <- window(AirPassengers, start = start, c(start, end))
timeSeries <- read.csv("./data/outputs/Myanmar_TownshipForestLossFragmentation.csv")
View(timeSeries)
?window
View(timeSeries)
timeSeries$Year[1]
max(((timeSeries$Year[2] - start)), 5) * 12
start <- timeSeries$Year[1]
max(((timeSeries$Year[2] - start)), 5) * 12
timeSeries$Year[2]
max(timeSeries$Year)
end <- max(timeSeries$Year)
period <- as.integer(1)
data <- window(timeSeries, start = start, c(start, end))
start <- timeSeries$Year[1] # year 1 = 2001
end <- max(timeSeries$Year)
data <- window(timeSeries, start = start, c(start, end))
data <- window(x = timeSeries, start = start, end = end))
data <- window(x = timeSeries, start = start, end = end)
data <- window(x = timeSeries, start = start, end = end, c(start, end))
modelFit <- arima(timeSeries, order = c(1, 0, 0), list(order = c(2,1, 0), period = 12))
data <- window(x = timeSeries, start = start, end = end)
# end <- max(timeSeries$Year)
end <- max(((timeSeries$Year[2] - start)), 5) * 12
data <- window(x = timeSeries, start = start, end = end, c(start, end))
data <- window(x = timeSeries, start = start, c(start, end))
end
# end <- max(timeSeries$Year)
end <- max(((max(timeSeries$Year) - start)), 5) * 12
end
end <-   max(((1958 - start)), 5) * 12
end
end <-   max(((1970 - start)), 5) * 12
end
end <-   max(((1951 - start)), 5) * 12
end
?max
min(5:1, pi
)
min(5:1, pi)
pmin(5:1, pi)
# end <- max(timeSeries$Year)
end <- max(((max(timeSeries$Year) - start)), 5) * 12
data <- window(x = timeSeries, start = start, c(start, end))
data <- window(x = timeSeries, start = start, c(start, 60))
data <- window(x = timeSeries, start = 2001, c(2001, 2017))
?window
presidents
class(presidents)
timeSeries <- as.ts(timeSeries)
data <- window(x = timeSeries, start = 2001, c(2001, 2017))
View(timeSeries)
?asts
?as.ts
country_loss <- read.csv("./data/outputs/Myanmar_TownshipForestLossFragmentation.csv")
View(country_loss)
unique(country_loss$Township)
# Create a list of township names
towns <- list(unique(country_loss$Township))
# Create a list of township names
towns <- unique(country_loss$Township)
class(towns)
# Create a list of township names
towns <- list(unique(country_loss$Township))
towns
towns[[2]]
towns[2]
# Create a list of township names
towns <- unique(country_loss$Township)
towns[[2]]
towns[[456]]
towns[200]
length(towns)
length(unique(country_loss$Township))
?append()
town_names <- unique(country_loss$Township)
# Create a list of datasets, one for each town
towns <- as.list()
for (i in 1:length(towns)) {
out <- country_loss %>%
filter(Township = town_names[[i]])
towns[[i]] <- out
}
library(tidyverse)
town_names <- unique(country_loss$Township)
# Create a list of datasets, one for each town
towns <- as.list()
for (i in 1:length(towns)) {
out <- country_loss %>%
filter(Township = town_names[[i]])
towns[[i]] <- out
}
# Create a list of datasets, one for each town
towns <- list()
for (i in 1:length(towns)) {
out <- country_loss %>%
filter(Township = town_names[[i]])
towns[[i]] <- out
}
for (i in 1:length(towns)) {
out <- country_loss %>%
filter(Township == town_names[[i]])
towns[[i]] <- out
}
town_names[[3]]
class(town_names[[3]])
for (i in 1:length(towns)) {
out <- country_loss %>%
filter(Township == as.character(town_names[[i]]))
towns[[i]] <- out
}
towns
towns[[2]]
towns[[1]]
town_names <- unique(country_loss$Township)
# Create a list of datasets, one for each town
towns <- list()
for (i in 1:length(town_names)) {
out <- country_loss %>%
filter(Township == as.character(town_names[[i]]))
towns[[i]] <- out
}
towns
towns[[3]]
?as.ts
town_names <- unique(country_loss$Township)
# Create a list of datasets, one for each town
towns <- list()
for (i in 1:length(town_names)) {
out <- country_loss %>%
filter(Township == as.character(town_names[[i]]))
towns[[i]] <- as.ts(out)
}
towns[[3]]
town_names[[3]]
data <- window(x = towns[[1]], start = 2001, c(2001, 2017))
towns[[1]]
modelFit <- arima(towns[[1]], order = c(1, 0, 0), list(order = c(2,1, 0), period = 12))
?arima
modelFit <- arima(towns[[1]], order = c(1, 0, 0),
seasonal = list(order = c(2, 1, 0), period = NA))
class(towns[[1]])
data <- towns[[1]]
class(towns[[1]])
class(data)
modelFit <- arima(data, order = c(1, 0, 0),
seasonal = list(order = c(2, 1, 0), period = NA))
town_names <- unique(country_loss$Township)
# Create a list of datasets, one for each town
towns <- list()
for (i in 1:length(town_names)) {
out <- country_loss %>%
filter(Township == as.character(town_names[[i]]))
towns[[i]] <- ts(out)
}
class(towns[[1]])
data <- towns[[1]]
modelFit <- arima(data, order = c(1, 0, 0),
seasonal = list(order = c(2, 1, 0), period = NA))
country_loss <- read.csv("./data/outputs/Myanmar_TownshipForestLossFragmentation.csv")
# Select down to just total area of patches lost
country_loss <- country_loss %>%
select(Township, Year, Total.Area.of.Loss.Patches)
town_names <- unique(country_loss$Township)
# Create a list of datasets, one for each town
towns <- list()
for (i in 1:length(town_names)) {
out <- country_loss %>%
filter(Township == as.character(town_names[[i]]))
towns[[i]] <- ts(out)
}
towns[[1]]
town_names <- unique(country_loss$Township)
# Create a list of datasets, one for each town
towns <- list()
for (i in 1:length(town_names)) {
out <- country_loss %>%
filter(Township == as.character(town_names[[i]])) %>%
select(Year, Total.Area.of.Loss.Patches)
towns[[i]] <- ts(out)
}
towns[[1]]
data <- towns[[1]]
modelFit <- arima(data, order = c(1, 0, 0),
seasonal = list(order = c(2, 1, 0), period = NA))
data
class(data)
# Create a list of township names
town_names <- unique(country_loss$Township)
# Create a list of datasets, one for each town
towns <- list()
for (i in 1:length(town_names)) {
out <- country_loss %>%
filter(Township == as.character(town_names[[i]])) %>%
select(Total.Area.of.Loss.Patches)
towns[[i]] <- ts(out)
}
data <- towns[[1]]
class(data)
modelFit <- arima(data, order = c(1, 0, 0),
seasonal = list(order = c(2, 1, 0), period = NA))
towns[[1]]
modelFit <- arima(data, order = c(1, 1, 0),
seasonal = list(order = c(2, 1, 0), period = NA))
modelFit <- arima(data, order = c(1, 0, 0),
seasonal = list(order = c(2, 1, 0), period = NA),
method= "ML")
modelFit
fit <- predict(modelFit, n.ahead = 17)
fit
?predict
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(tidyr)
library(data.table)
library(RcolorBrewer)
library(RcolorBrewer)
library(doParallel)
library(scales)
#Load data: deforestation in Myanmar as determined by Hansen 2016
#Deforestation is given in Landsat pixels (30m x 30m)
Myanmar_table = read.csv("towndeforestation_total.csv")
getwd()
=======
library(dplyr)
library(sp)
library(raster)
library(rgeos)
library(rgdal)
library(SDMTools)
options(stringsAsFactors = F)
projected <-'+proj=eqc +lat_ts=0 +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs'
townships <- readOGR(dsn = "./data/inputs/shapefiles/townships", layer = "MMR_adm3")
Hansen1 <-raster("./data/inputs/rasters/Hansen_loss_year/Hansen_GFC-2017-v1.5_lossyear_30N_090E.tif")
Hansen2 <-raster("./data/inputs/rasters/Hansen_loss_year/Hansen_GFC-2017-v1.5_lossyear_20N_090E.tif")
Hansen3 <-raster("./data/inputs/rasters/Hansen_loss_year/Hansen_GFC-2017-v1.5_lossyear_10N_090E.tif")
Hansen4 <-raster("./data/inputs/rasters/Hansen_loss_year/Hansen_GFC-2017-v1.5_lossyear_30N_100E.tif")
Hansen <-raster::merge(Hansen1,Hansen2,Hansen3,Hansen4)
writeRaster(Hansen, "./data/inputs/rasters/Hansen_loss_year/All_Hansen.tif", format = "GTiff")
Hansen <-raster("./data/inputs/rasters/Hansen_loss_year/All_Hansen.tif") %>%
projectRaster(crs = crs(projected))
##### Calculating Total Loss #####
for (i in 1:nrow(townships)) {
output <-data.frame() # have it output for each state so that
# I don't lose progress if something breaks.
township <-townships[i,]
targetHansen <- crop(Hansen, township)
# targetHansen <-projectRaster(from = targetHansen, res = 30, crs = crs(projected), method = 'ngb')
# ^projection made the process very slow so commented out
for (j in 1:17){ # years in the dataset are flagged as 1-17
print(j)
to<-rep(NA, 18)
to[j+1]<-1
reclassify<-cbind(c(0:17), to)
colnames(reclassify)<-c('is', 'becomes')
if(j %in% targetHansen[]) {
townshipLoss<-reclassify(targetHansen, rcl = reclassify)
# Can't use dplyr pipelines here because SDMTools::ConnCompLabel
# and SDMTools::PatchStat don't accept them
patches<- ConnCompLabel(townshipLoss)
# Selecting just area from patchstat, summing
patches<-PatchStat(patches, cellsize = 30, latlon = T)[,6]%>%as.data.frame()
rowOutput<-c(township@data$NAME_3, j, nrow(patches), mean(patches[,1]), sum(patches[,1]))
} else {
rowOutput<-c(township@data$NAME_3, j, 0,0,0)
}
output<-rbind(output, rowOutput)
colnames(output)<-c("Township", "Year",
"Number of Loss Patches", "Avg Patch Size", "Total Area of Loss Patches")
}
if(file.exists(paste0("./data/intermediate/township_level/", township@data$NAME_3, ".csv"))){
write.csv(output, paste0("./data/intermediate/township_level/", township@data$NAME_3, "2.csv"))
} else {
write.csv(output, paste0("./data/intermediate/township_level/", township@data$NAME_3, ".csv"))
}
}
Hansen1 <-raster("./data/inputs/rasters/Hansen_loss_year/Hansen_GFC-2017-v1.5_lossyear_30N_090E.tif")
Hansen <-raster("./data/inputs/rasters/Hansen_loss_year/All_Hansen.tif") %>%
projectRaster(crs = crs(projected))
projected <-'+proj=eqc +lat_ts=0 +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs'
townships <- readOGR(dsn = "./data/inputs/shapefiles/townships", layer = "MMR_adm3")
library(dplyr)
library(sp)
library(raster)
library(rgeos)
library(rgdal)
library(SDMTools)
Hansen <-raster("./data/inputs/rasters/Hansen_loss_year/All_Hansen.tif") %>%
projectRaster(crs = crs(projected))
##### Calculating Total Loss #####
for (i in 1:nrow(townships)) {
output <-data.frame() # have it output for each state so that
# I don't lose progress if something breaks.
township <-townships[i,]
targetHansen <- crop(Hansen, township)
# targetHansen <-projectRaster(from = targetHansen, res = 30, crs = crs(projected), method = 'ngb')
# ^projection made the process very slow so commented out
for (j in 1:17){ # years in the dataset are flagged as 1-17
print(j)
to<-rep(NA, 18)
to[j+1]<-1
reclassify<-cbind(c(0:17), to)
colnames(reclassify)<-c('is', 'becomes')
if(j %in% targetHansen[]) {
townshipLoss<-reclassify(targetHansen, rcl = reclassify)
# Can't use dplyr pipelines here because SDMTools::ConnCompLabel
# and SDMTools::PatchStat don't accept them
patches<- ConnCompLabel(townshipLoss)
# Selecting just area from patchstat, summing
patches<-PatchStat(patches, cellsize = 30, latlon = T)[,6]%>%as.data.frame()
rowOutput<-c(township@data$NAME_3, j, nrow(patches), mean(patches[,1]), sum(patches[,1]))
} else {
rowOutput<-c(township@data$NAME_3, j, 0,0,0)
}
output<-rbind(output, rowOutput)
colnames(output)<-c("Township", "Year",
"Number of Loss Patches", "Avg Patch Size", "Total Area of Loss Patches")
}
if(file.exists(paste0("./data/intermediate/township_level/", township@data$NAME_3, ".csv"))){
write.csv(output, paste0("./data/intermediate/township_level/", township@data$NAME_3, "2.csv"))
} else {
write.csv(output, paste0("./data/intermediate/township_level/", township@data$NAME_3, ".csv"))
}
}
##### Data Clean-up #####
townships<-list.files("./data/intermediate/township_level", pattern = ".csv", full.names = T)
fragstats<-data.frame()
for (township in townships) {
current<-read.csv(township)
current<-current[,-c(1)] #removes an unnecessary objectid column
for (i in 1:nrow(current)){ #get the actual year
if(nchar(current$Year[i]) == 1) {
current$Year[i]<-paste0('200', i)
}else{
current$Year[i]<-paste0('20', i)
}
}
fragstats<-rbind(fragstats,current)
}
write.csv(fragstats, "./data/outputs/township_level/Myanmar_TownshipForestLoss.csv")
>>>>>>> 74123ee1ddae913b5cbc2dd6650ad6c67aa4e8ec
